# Brain Art - Dokumentacja Rozwojowa

**Dla deweloper√≥w i kontrybutor√≥w**

---

## üìê Architektura Systemu

### Przep≈Çyw Danych

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    BLE     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    LSL     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Muse S   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ OpenMuse ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ Python   ‚îÇ
‚îÇ Athena   ‚îÇ            ‚îÇ  stream  ‚îÇ            ‚îÇ   App    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                        ‚îÇ                             ‚îÇ
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ   EEG   ‚îÇ                  ‚îÇ  Brain  ‚îÇ
                   ‚îÇProcessor‚îÇ                  ‚îÇ  Viz    ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ                             ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
                                  ‚îÇDisplay‚îÇ
                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Modu≈Çy

#### 1. `muse_connector.py` - Akwizycja Danych

**Odpowiedzialno≈õƒá:**
- Po≈ÇƒÖczenie z Muse S przez LSL
- Strumieniowanie EEG (256 Hz, 4 kana≈Çy)
- Monitoring jako≈õci sygna≈Çu

**API:**
```python
connector = MuseConnector(mode='lsl', enable_motion=True)
connector.connect()

# Pobierz dane
data = connector.get_eeg_data(duration=1.0)  # ndarray [samples, channels]
quality = connector.get_signal_quality()     # list [n_channels] (0-100%)

connector.disconnect()
```

**Implementacja:**
- U≈ºywa `mne_lsl.lsl` (StreamInlet)
- Fallback na `pylsl` je≈õli brak mne_lsl
- Dynamiczne pobieranie danych (duration parametr w `get_eeg_data()`)
- Timeout: 5s na connect
- Monitoring jako≈õci sygna≈Çu przez `SignalQualityChecker`
- Obs≈Çuga motion data (ACC/GYRO) przez `get_motion_data()`

#### 2. `eeg_processor.py` - Przetwarzanie Sygna≈Çu

**Odpowiedzialno≈õƒá:**
- FFT i analiza mocy czƒôstotliwo≈õciowej
- Notch filtering (50Hz/60Hz - zak≈Ç√≥cenia sieciowe)
- Kalibracja (baseline)
- Obliczanie metryk (attention, relaxation)

**API:**
```python
processor = EEGProcessor()  # Parametry z config.py (SAMPLE_RATE, WINDOW_SIZE)

# Dodaj dane
processor.add_data(eeg_array)  # [samples, 4] - u≈ºywa tylko pierwszych 4 kana≈Ç√≥w

# Kalibracja
# UWAGA: W main.py kalibracja jest wykonywana rƒôcznie, nie u≈ºywa processor.calibrate()
# W main.py: zbiera dane przez config.CALIBRATION_TIME sekund, potem ustawia baseline
processor.baseline = powers.copy()  # Rƒôczne ustawienie baseline
processor.is_calibrated = True

# Przetwarzanie
powers = processor.compute_band_powers()  # dict {band: power} - u≈õrednione po kana≈Çach
powers_per_ch = processor.compute_band_powers_per_channel()  # dict {band: [ch0, ch1, ch2, ch3]}
attention = processor.compute_attention()      # float 0-1
relaxation = processor.compute_relaxation()    # float 0-1
```

**Implementacja:**
- **Pasma czƒôstotliwo≈õci (obliczane):**
  - Delta: 1-4 Hz (sen g≈Çƒôboki) - **obliczane, ale nie u≈ºywane do wizualizacji**
  - Theta: 4-8 Hz (medytacja, senno≈õƒá) - **u≈ºywane w relaxation (20%)**
  - Alpha: 8-13 Hz (relaksacja) - **g≈Ç√≥wny wska≈∫nik relaxation (80%)**
  - Beta: 13-30 Hz (uwaga) - **g≈Ç√≥wny wska≈∫nik attention (70%)**
  - Gamma: 30-44 Hz (przetwarzanie) - **u≈ºywane w attention (30%)**

- **Uwaga:** Program oblicza wszystkie 5 pasm, ale do wizualizacji u≈ºywa **tylko 4 pasm** (alpha, beta, gamma, theta). Delta jest obliczane i dostƒôpne w `band_powers`, ale nie jest u≈ºywane w metrykach attention/relaxation.
  
- **Metryki (u≈ºywane do wizualizacji):**
  - `attention = 0.7 * (beta / beta_baseline) + 0.3 * (gamma / gamma_baseline)`
    - U≈ºywa: **beta (70%) + gamma (30%)**
    - Normalizacja wzglƒôdem baseline, clip do [0, 2], skalowanie do [0, 1]
  - `relaxation = 0.8 * (alpha / alpha_baseline) + 0.2 * (theta / theta_baseline)`
    - U≈ºywa: **alpha (80%) + theta (20%)**
    - Normalizacja wzglƒôdem baseline, clip do [0, 2], skalowanie do [0, 1]
  - Smoothing: rolling average (5 pr√≥bek w `metric_history`)

- **Przetwarzanie sygna≈Çu:**
  - Detrend (usuwanie trendu)
  - Notch filter 50Hz/60Hz (zak≈Ç√≥cenia sieciowe) - `scipy.signal.iirnotch`
  - Hanning window (wyg≈Çadzanie przed FFT)
  - FFT (Fast Fourier Transform) - bezpo≈õrednia analiza czƒôstotliwo≈õciowa
  - **UWAGA:** Filtry Butterworth sƒÖ tworzone w `_create_filters()`, ale nie sƒÖ u≈ºywane w `compute_band_powers()` - u≈ºywa siƒô bezpo≈õrednio FFT

#### 3. `brain_visualizer.py` - Wizualizacja

**Odpowiedzialno≈õƒá:**
- System czƒÖsteczek (particles)
- 3 tryby: Alpha, Beta, Mixed
- Rendering (pygame)
- HUD/info

**API:**
```python
viz = BrainVisualizer()  # Parametry z config.py (WINDOW_WIDTH, WINDOW_HEIGHT, FULLSCREEN)

# Ustaw metryki
viz.set_metrics(attention=0.75, relaxation=0.5, motion_metrics=None)  # motion_metrics opcjonalne

# Zmie≈Ñ tryb
viz.set_mode('mixed')  # 'alpha', 'beta', 'mixed'

# Renderuj klatkƒô
viz.run_frame()  # Jedna klatka @ TARGET_FPS (domy≈õlnie 30 FPS z config)

# Screenshot
viz.save_screenshot()

# Cleanup
viz.close()
```

**Implementacja:**

- **Particle System:**
  ```python
  class Particle:
      position: (x, y)
      velocity: (vx, vy)
      color: (r, g, b)
      size: float
      lifetime: float  # seconds
  ```

- **Spawning:**
  - Rate zale≈ºny od attention/relaxation
  - Pozycja: losowa (tryb beta) lub centrum (alpha)
  - Kolor: z palety odpowiedniej dla trybu
  - Rozmiar: 8-40 px

- **Rendering:**
  - Fade effect: alpha blend
  - Draw particles: pygame.draw.circle (z cache dla wydajno≈õci)
  - Particle cache: pre-rendered surfaces dla lepszego FPS
  - HUD: FPS, metrics, mode, motion status
  - Auto-optimization: dostosowanie MAX_PARTICLES i FPS target na podstawie rzeczywistego FPS

- **Motion Effects (opcjonalne):**
  - Head tilt affects particle direction (95% influence przy silnym tilt)
  - Motion intensity affects particle count (je≈õli `MOTION_INTENSITY_SCALING=True` w config, domy≈õlnie: False)
  - Gestures: nod (change mode), shake (clear screen)

#### 4. `main.py` - G≈Ç√≥wna Aplikacja

**Przep≈Çyw:**
```python
1. Setup
   - Po≈ÇƒÖcz z Muse (MuseConnector)
   - Inicjalizuj EEG processor (EEGProcessor)
   - Inicjalizuj motion processor (MotionProcessor, opcjonalnie)
   - Inicjalizuj performance optimizer (PerformanceOptimizer, opcjonalnie)
   - Inicjalizuj brain visualizer (BrainVisualizer)
   - Inicjalizuj EEG monitor (EEGVisualizer, opcjonalnie)

2. Calibration
   - Zbierz dane przez config.CALIBRATION_TIME sekund (domy≈õlnie 5s)
   - Wy≈õwietl instrukcje i progress
   - Rƒôcznie ustaw baseline: processor.baseline = powers.copy()
   - Ustaw processor.is_calibrated = True
   # UWAGA: NIE u≈ºywa processor.calibrate() - zbiera dane rƒôcznie

3. Main Loop
   while running:
       # Motion data (co 100ms - czƒô≈õciej ni≈º EEG)
       if motion_enabled:
           - Pobierz motion data (ACC/GYRO)
           - Dodaj do motion_processor
           - Wykryj gesty (nod ‚Üí cycle_mode, shake ‚Üí clear_screen)
       
       # EEG data (co UPDATE_INTERVAL ms, domy≈õlnie 500ms = 2Hz)
       if time_to_update_eeg:
           - Pobierz EEG data
           - Przetw√≥rz (async w thread je≈õli optimizer w≈ÇƒÖczony)
           - Oblicz band powers, attention, relaxation
           - Aktualizuj brain visualizer
           - Aktualizuj EEG monitor (je≈õli w≈ÇƒÖczony)
       
       # Render
       - Renderuj klatkƒô wizualizacji
       - Update EEG monitor plots (je≈õli w tym samym procesie)
       - Handle events (keyboard, mouse)

4. Cleanup
   - Roz≈ÇƒÖcz Muse
   - Zamknij visualizers
   - Cleanup optimizer (threads, processes)
```

#### 5. `motion_processor.py` - Przetwarzanie Ruchu

**Odpowiedzialno≈õƒá:**
- Przetwarzanie danych ACC/GYRO z Muse S
- Wykrywanie gest√≥w (nod, shake)
- Wykrywanie nachylenia g≈Çowy (tilt)
- Obliczanie motion metrics

**API:**
```python
processor = MotionProcessor(sample_rate=52)  # 52 Hz dla ACC/GYRO

# Dodaj dane
processor.add_data(acc_data, gyro_data)  # acc: [X, Y, Z] g, gyro: [X, Y, Z] deg/s

# Wykrywanie gest√≥w
if processor.detect_nod():      # Skiniƒôcie g≈ÇowƒÖ (prz√≥d-d√≥≈Ç)
    # Zmie≈Ñ tryb wizualizacji
if processor.detect_shake():    # PotrzƒÖsanie g≈ÇowƒÖ (lewo-prawo)
    # Wyczy≈õƒá ekran

# Nachylenie g≈Çowy
tilt_lr, tilt_fb = processor.get_head_tilt()  # -1 do 1

# Metryki
metrics = processor.get_metrics()  # dict z tilt, motion_intensity, etc.
```

**Implementacja:**
- **Gest detection:**
  - Nod: du≈ºa zmiana w ACC X (threshold: 0.8g, test: 1.4g)
  - Shake: szybka rotacja GYRO Z (threshold: 150¬∞/s, test: 245¬∞/s)
  - Cooldown: 1.5s miƒôdzy gestami

- **Head tilt:**
  - Tilt left-right: ACC Y (roll)
  - Tilt forward-backward: ACC X (pitch)
  - U≈ºywane do modyfikacji kierunku czƒÖsteczek

- **Motion metrics:**
  - `motion_intensity`: 0-1 (na podstawie std dev ACC/GYRO)
  - `tilt_left_right`: -1 (lewo) do +1 (prawo)
  - `tilt_forward_backward`: -1 (prz√≥d) do +1 (ty≈Ç)

#### 6. `eeg_visualizer.py` - Monitor EEG

**Odpowiedzialno≈õƒá:**
- Wizualizacja sygna≈Ç√≥w EEG w osobnym oknie
- Topomapy (mapy topograficzne aktywno≈õci)
- Raw EEG traces (opcjonalnie)
- Power spectrogram

**API:**
```python
# Factory function
viz = create_eeg_visualizer(use_advanced=True, buffer_duration=5.0)

# Setup
viz.setup_window()

# Update data
viz.update_data(eeg_data, band_powers, band_powers_per_channel)

# Render (w g≈Ç√≥wnej pƒôtli)
viz.update_plots()

# Cleanup
viz.close()
```

**Implementacja:**
- U≈ºywa matplotlib (TkAgg backend)
- Wymaga MNE dla topomap√≥w (fallback: SimpleEEGVisualizer)
- Mo≈ºe dzia≈Çaƒá w osobnym procesie (`EEGVisualizerProcess`) dla lepszego FPS
- Per-channel band powers dla dok≈Çadniejszych topomap√≥w

#### 7. `performance_optimizer.py` - Optymalizacja Wydajno≈õci

**Odpowiedzialno≈õƒá:**
- Multithreading dla oblicze≈Ñ EEG
- Multiprocessing dla monitora EEG
- GPU acceleration
- Cache management

**API:**
```python
optimizer = PerformanceOptimizer(processor=eeg_processor)

# Async processing
optimizer.process_eeg_async(eeg_data)

# Get results (non-blocking)
results = optimizer.get_eeg_results()  # dict lub None
if results:
    band_powers = results['band_powers']
    attention = results['attention']
    relaxation = results['relaxation']

# Cleanup
optimizer.cleanup()
```

**Implementacja:**
- `EEGComputeThread`: osobny wƒÖtek dla oblicze≈Ñ FFT (nie blokuje g≈Ç√≥wnej pƒôtli)
- `EEGVisualizerProcess`: osobny proces dla monitora EEG (znaczny wzrost FPS)
- Thread pools i process pools dla r√≥wnoleg≈Çych oblicze≈Ñ

#### 8. `signal_quality.py` - Ocena Jako≈õci Sygna≈Çu

**Odpowiedzialno≈õƒá:**
- Ocena jako≈õci sygna≈Çu EEG
- Metryki per-kana≈Ç i og√≥lna
- Wykrywanie artefakt√≥w i zak≈Ç√≥ce≈Ñ

**API:**
```python
checker = SignalQualityChecker(sample_rate=256)

# Ocena
result = checker.assess_quality(eeg_data)  # dict z metrics, warnings, etc.
# result['overall_quality']: 0-100
# result['channel_quality']: [0-100] per channel
# result['warnings']: list of warnings

# Szybka ocena (dla g≈Ç√≥wnej pƒôtli)
quality = quick_quality_check(eeg_data)  # 0-100
```

**Implementacja:**
- **Metryki:**
  - Variance (wariancja sygna≈Çu)
  - Amplitude (amplituda peak-to-peak)
  - Alpha power (obecno≈õƒá fal alpha)
  - Line noise (zak≈Ç√≥cenia 50Hz/60Hz)
  - Artifacts (kurtosis, gradient analysis)
  - Stationarity (stabilno≈õƒá sygna≈Çu)

- **Wagi:**
  - Variance: 30%
  - Amplitude: 20%
  - Alpha power: 15%
  - Line noise: 15%
  - Artifacts: 15%
  - Stationarity: 5%

#### 9. `config.py` - Konfiguracja

**Sekcje:**
```python
# Muse Connection
CONNECTION_MODE = 'lsl'  # 'lsl' (recommended) or 'bluetooth'
MUSE_ADDRESS = '00:55:DA:B9:FB:2C'  # MAC address (for Bluetooth mode)
MUSE_NAME = 'Muse-S'  # Device name (for muselsl stream)

# EEG Processing
SAMPLE_RATE = 256
WINDOW_SIZE = 64
CALIBRATION_TIME = 5

# Visualization
WINDOW_WIDTH = 1280
WINDOW_HEIGHT = 720
FULLSCREEN = False
TARGET_FPS = 30  # Default target FPS
UPDATE_INTERVAL = 500  # ms - how often to update EEG data (2 Hz)

# Particles
PARTICLE_LIFETIME = 2.0
PARTICLE_SIZE_MIN = 8
PARTICLE_SIZE_MAX = 40
PARTICLE_SPEED_MIN = 30
PARTICLE_SPEED_MAX = 100
MAX_PARTICLES = 150  # Default (auto-optimized based on FPS)

# Colors
COLOR_PALETTES = {...}

# Debug
DEBUG = True
SHOW_FPS = True
SHOW_SIGNAL_QUALITY = True
SHOW_EEG_MONITOR = False  # Auto-open EEG monitor window
EEG_MONITOR_SHOW_RAW_TRACES = False  # Show raw traces in monitor
DEBUG_MOTION = False  # Show detailed motion data

# Motion Features
ENABLE_MOTION = True  # Enable motion features (requires preset with ACC/GYRO)
MOTION_GESTURE_CONTROL = True  # Gestures: nod (change mode), shake (clear)
MOTION_TILT_EFFECTS = True  # Head tilt affects particle direction
MOTION_INTENSITY_SCALING = False  # Motion intensity affects particle count
```

---

## üî¨ Algorytmy

### FFT (Fast Fourier Transform)

**Cel:** Przekszta≈Çƒá sygna≈Ç czasowy ‚Üí czƒôstotliwo≈õciowy

```python
from scipy.fft import fft, fftfreq

# Input: signal [256 samples]
n = len(signal)
fft_vals = fft(signal)
fft_freqs = fftfreq(n, 1/sample_rate)

# Tylko dodatnie czƒôstotliwo≈õci
pos_mask = fft_freqs > 0
freqs = fft_freqs[pos_mask]

# Power spectrum (znormalizowane przez N)
power_spectrum = (np.abs(fft_vals[pos_mask]) / n) ** 2

# Power w pa≈õmie [f1, f2]
band_mask = (freqs >= f1) & (freqs <= f2)
power = np.mean(power_spectrum[band_mask])
```

### Normalizacja

**Cel:** Wzglƒôdne zmiany vs baseline

```python
# Podczas kalibracji (w main.py)
powers = processor.compute_band_powers()
processor.baseline = powers.copy()  # Ustawienie baseline

# Podczas u≈ºywania
normalized_power = current_power / baseline_power  # Normalizacja wzglƒôdna

# Dla metryk attention/relaxation:
# 1. Normalizacja wzglƒôdem baseline
beta_norm = beta / beta_baseline
gamma_norm = gamma / gamma_baseline

# 2. Kombinacja wa≈ºona
attention_value = 0.7 * beta_norm + 0.3 * gamma_norm

# 3. Clip do [0, 2] i skalowanie do [0, 1]
attention_value = np.clip(attention_value, 0, 2) / 2.0

# 4. Smoothing (rolling average)
metric_history['attention'].append(attention_value)
attention_smooth = np.mean(metric_history['attention'])
```

### Smoothing

**Cel:** Redukcja szumu, p≈Çynne zmiany

```python
from collections import deque

history = deque(maxlen=5)  # Rolling window
history.append(new_value)
smoothed = np.mean(history)
```

---

## ‚öôÔ∏è Parametry i Tuning

### Performance

**Je≈õli FPS < 30:**
```python
WINDOW_SIZE = 64           # ‚Üì FFT szybsze
UPDATE_INTERVAL = 500      # ‚Üë Rzadziej EEG
MAX_PARTICLES = 300        # ‚Üì Mniej czƒÖsteczek
PARTICLE_LIFETIME = 1.0    # ‚Üì Kr√≥tsze ≈ºycie
```

**Je≈õli CPU > 80%:**
- Zwiƒôksz `UPDATE_INTERVAL`
- Zmniejsz `MAX_PARTICLES`
- Wy≈ÇƒÖcz `DEBUG`

### Responsiveness

**Je≈õli za wolna reakcja:**
```python
# eeg_processor.py
self.metric_history = {
    'attention': deque(maxlen=2),    # by≈Ço 5
    'relaxation': deque(maxlen=2),
}
```

**Je≈õli za szybka (niestabilna):**
```python
maxlen=10  # Wiƒôcej wyg≈Çadzania
```

### Visual Density

**Za ma≈Ço czƒÖsteczek:**
```python
MAX_PARTICLES = 1000
PARTICLE_LIFETIME = 3.0
PARTICLE_SIZE_MAX = 50
```

**Za du≈ºo:**
```python
MAX_PARTICLES = 200
PARTICLE_LIFETIME = 0.5
```

---

## üöÄ Rozszerzenia

### 1. Zapis Danych EEG

```python
# W main.py
import csv

csv_file = open('eeg_recording.csv', 'w', newline='')
writer = csv.writer(csv_file)
writer.writerow(['time', 'TP9', 'AF7', 'AF8', 'TP10', 'attention', 'relaxation'])

# W g≈Ç√≥wnej pƒôtli
writer.writerow([time.time(), *eeg_data[-1], attention, relaxation])

# Cleanup
csv_file.close()
```

### 2. Analiza Nagranych Danych

```python
import pandas as pd
import matplotlib.pyplot as plt

# Wczytaj
df = pd.read_csv('eeg_recording.csv')

# Plot
fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# EEG
df.plot(x='time', y=['TP9', 'AF7', 'AF8', 'TP10'], ax=axes[0])
axes[0].set_title('Raw EEG')

# Metrics
df.plot(x='time', y=['attention', 'relaxation'], ax=axes[1])
axes[1].set_title('Computed Metrics')

plt.tight_layout()
plt.show()
```

### 3. Muzyka z EEG

```python
import numpy as np
import pygame.mixer

pygame.mixer.init(frequency=22050, size=-16, channels=1)

def generate_tone(frequency, duration, volume):
    sample_rate = 22050
    n_samples = int(sample_rate * duration)
    t = np.linspace(0, duration, n_samples)
    wave = np.sin(2 * np.pi * frequency * t) * volume
    wave = (wave * 32767).astype(np.int16)
    return pygame.sndarray.make_sound(wave)

# W g≈Ç√≥wnej pƒôtli
freq = 200 + attention * 400  # 200-600 Hz
volume = relaxation * 0.5
tone = generate_tone(freq, 0.1, volume)
tone.play()
```

### 4. OSC do Tabletu

```python
from pythonosc import udp_client

# Setup
osc_client = udp_client.SimpleUDPClient("192.168.1.100", 5005)

# W pƒôtli
osc_client.send_message("/brain/attention", attention)
osc_client.send_message("/brain/relaxation", relaxation)
osc_client.send_message("/brain/alpha", alpha_power)
osc_client.send_message("/brain/beta", beta_power)
```

**Na tablecie (TouchOSC/Processing):**
```processing
import oscP5.*;

OscP5 osc;

void setup() {
  osc = new OscP5(this, 5005);
}

void oscEvent(OscMessage msg) {
  if (msg.checkAddrPattern("/brain/attention")) {
    float attention = msg.get(0).floatValue();
    // Rysuj co≈õ...
  }
}
```

### 5. W≈Çasne Wizualizacje

**Linie zamiast czƒÖsteczek:**
```python
# W brain_visualizer.py
def _draw_lines(self):
    if random.random() < 0.1:
        start = (random.randint(0, self.width), 0)
        end = (random.randint(0, self.width), self.height)
        color = self._get_color_for_mode()
        thickness = int(5 + self.attention * 10)
        pygame.draw.line(self.canvas, color, start, end, thickness)
```

**Fraktale:**
```python
def draw_fractal_tree(surface, x, y, angle, length, attention):
    if length < 5:
        return

    end_x = x + length * np.cos(np.radians(angle))
    end_y = y + length * np.sin(np.radians(angle))

    color = (255 * attention, 100, 255 * (1 - attention))
    pygame.draw.line(surface, color, (x, y), (end_x, end_y), 2)

    draw_fractal_tree(surface, end_x, end_y, angle - 20, length * 0.7, attention)
    draw_fractal_tree(surface, end_x, end_y, angle + 20, length * 0.7, attention)
```

### 6. Multiplayer (2 Muse)

```python
# Dwa connectory
muse1 = MuseConnector(address='00:55:DA:B9:FB:2C')
muse2 = MuseConnector(address='00:55:DA:B9:XX:XX')

# Dwa procesory
proc1 = EEGProcessor()
proc2 = EEGProcessor()

# W pƒôtli
att1 = proc1.compute_attention()
att2 = proc2.compute_attention()

# Tryby:
# 1. Wsp√≥≈Çpraca: avg_attention = (att1 + att2) / 2
# 2. Rywalizacja: winner = "P1" if att1 > att2 else "P2"
# 3. Duel: ka≈ºdy kontroluje po≈Çowƒô ekranu
```

---

## üß™ Testing

### Uruchamianie Test√≥w

Wszystkie testy znajdujƒÖ siƒô w katalogu `tests/`. Uruchamiaj je indywidualnie:

```bash
# Kompleksowy test systemu (zalecane jako pierwszy)
python tests/test_system.py

# Test funkcjonalno≈õci monitora EEG
python tests/test_eeg_visualizer.py

# Testy optymalizacji wydajno≈õci
python tests/test_performance.py

# Testy gest√≥w ruchowych (wymaga aktywnego strumienia OpenMuse)
python tests/gestures/test_motion.py
python tests/gestures/test_motion_axes.py
python tests/gestures/test_tilt.py
```

### Zestawy Test√≥w

- **test_system.py**: Kompleksowy test komponent√≥w z danymi syntetycznymi
  - Testuje wszystkie g≈Ç√≥wne modu≈Çy (MuseConnector, EEGProcessor, BrainVisualizer)
  - Weryfikuje importy i zale≈ºno≈õci
  - Opcjonalny test po≈ÇƒÖczenia OpenMuse
  - Nie wymaga urzƒÖdzenia (u≈ºywa danych syntetycznych)

- **test_eeg_visualizer.py**: Test funkcjonalno≈õci okna monitora EEG
  - Testuje wizualizator EEG standalone
  - Weryfikuje komponenty wizualizacji

- **test_performance.py**: Testy optymalizacji wydajno≈õci
  - Informacje o CPU
  - Threading i multiprocessing
  - Testy FPS pygame
  - Rekomendacje konfiguracji

- **gestures/** - Testy funkcji ruchowych:
  - `test_motion.py`: Testy wykrywania gest√≥w (skiniƒôcie, potrzƒÖsanie)
  - `test_motion_axes.py`: Identyfikacja osi akcelerometru/≈ºyroskopu
  - `test_tilt.py`: Testy wykrywania nachylenia g≈Çowy
  - **Uwaga**: WymagajƒÖ aktywnego strumienia OpenMuse z danymi ruchowymi (preset p20, p21, p1041)

- **debug_channels.py**: Analiza strumieni LSL i inspekcja kana≈Ç√≥w
  - Wy≈õwietla dostƒôpne strumienie LSL
  - Pokazuje nazwy kana≈Ç√≥w i ich kolejno≈õƒá
  - Przydatne do rozwiƒÖzywania problem√≥w z po≈ÇƒÖczeniem

### Unit Tests

Przyk≈Çad struktury test√≥w jednostkowych:

```python
# test_eeg_processor.py
import unittest
import numpy as np
from eeg_processor import EEGProcessor

class TestEEGProcessor(unittest.TestCase):
    def setUp(self):
        self.proc = EEGProcessor()

    def test_add_data(self):
        data = np.random.randn(256, 4)
        self.proc.add_data(data)
        self.assertEqual(self.proc.buffer.shape[0], 256)

    def test_compute_band_powers(self):
        # Simulate alpha wave (10 Hz)
        t = np.linspace(0, 1, 256)
        signal = np.sin(2 * np.pi * 10 * t)
        data = np.tile(signal, (4, 1)).T

        self.proc.add_data(data)
        powers = self.proc.compute_band_powers()

        # Alpha should be dominant
        self.assertGreater(powers['alpha'], powers['beta'])
```

### Integration Tests

```bash
# test_system.py (ju≈º istnieje)
python tests/test_system.py
```

### Manual Testing Checklist

- [ ] Po≈ÇƒÖczenie z Muse
- [ ] Jako≈õƒá sygna≈Çu > 70%
- [ ] Kalibracja (5s domy≈õlnie)
- [ ] Zamknij oczy ‚Üí wiƒôcej alpha ‚Üí ciep≈Çe kolory
- [ ] Otw√≥rz oczy + liczenie ‚Üí wiƒôcej beta ‚Üí jasne kolory
- [ ] Motion gestures dzia≈ÇajƒÖ (nod ‚Üí zmiana trybu, shake ‚Üí wyczyszczenie)
- [ ] Head tilt wp≈Çywa na kierunek czƒÖsteczek
- [ ] EEG monitor dzia≈Ça (je≈õli w≈ÇƒÖczony)
- [ ] Screenshoty zapisujƒÖ siƒô
- [ ] FPS > 30 (lub dostosowany do PERFORMANCE_MODE)
- [ ] Brak memory leaks (d≈Çuga sesja)

---

## üìä Metryki i Monitoring

### Performance Metrics

```python
# W main.py
import time

frame_times = []

while running:
    start = time.time()

    # ... g≈Ç√≥wna pƒôtla ...

    frame_time = time.time() - start
    frame_times.append(frame_time)

    if len(frame_times) > 100:
        avg_fps = 1.0 / np.mean(frame_times)
        print(f"Avg FPS: {avg_fps:.1f}")
        frame_times = []
```

### Signal Quality Monitoring

```python
# W muse_connector.py
from src.signal_quality import SignalQualityChecker

# Inicjalizacja
self.quality_checker = SignalQualityChecker(sample_rate=256)

# Aktualizacja jako≈õci (w _update_signal_quality)
quality_result = self.quality_checker.assess_quality(eeg_data)
self.signal_quality = quality_result['channel_quality']  # [0-100] per channel
self.overall_quality = quality_result['overall_quality']  # 0-100
self.quality_warnings = quality_result['warnings']  # List of warnings

# Pobranie jako≈õci
quality_list = connector.get_signal_quality()  # [0-100] per channel
overall = connector.get_overall_quality()  # 0-100
warnings = connector.get_quality_warnings()  # List of strings
```

**Metryki jako≈õci:**
- Variance: 30% (wariancja sygna≈Çu)
- Amplitude: 20% (peak-to-peak)
- Alpha power: 15% (obecno≈õƒá fal alpha)
- Line noise: 15% (zak≈Ç√≥cenia 50Hz/60Hz)
- Artifacts: 15% (ruchy, mruganie)
- Stationarity: 5% (stabilno≈õƒá)

---

## üêõ Debugging

### Enable Debug Mode

```python
# config.py
DEBUG = True
SHOW_FPS = True
SHOW_SIGNAL_QUALITY = True
LOG_LEVEL = 'DEBUG'
```

### Logging

```python
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler('brain_art.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
logger.debug("This is a debug message")
```

### Common Issues

**FFT errors:**
```python
# Sprawd≈∫ rozmiar okna
assert len(signal) == WINDOW_SIZE
# Sprawd≈∫ czy sƒÖ NaN
assert not np.any(np.isnan(signal))
```

**LSL connection:**
```python
# Test resolve
from mne_lsl.lsl import resolve_streams
streams = resolve_streams(timeout=5)
print(f"Found {len(streams)} streams")
```

---

## üìö Zasoby

### Dokumentacja
- **OpenMuse:** https://github.com/DominiqueMakowski/OpenMuse
- **MNE-LSL:** https://mne.tools/mne-lsl/
- **Pygame:** https://www.pygame.org/docs/
- **SciPy Signal:** https://docs.scipy.org/doc/scipy/reference/signal.html

### Naukowe
- **EEG Bands:** https://en.wikipedia.org/wiki/Electroencephalography
- **BCI:** https://www.frontiersin.org/journals/human-neuroscience
- **FFT:** https://en.wikipedia.org/wiki/Fast_Fourier_transform

### Inspiracje
- NeuroSky MindWave
- InteraXon Muse Apps
- BCI Art Projects
